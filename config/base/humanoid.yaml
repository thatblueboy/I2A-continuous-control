expt_name: None

environment:
  name: "Humanoid-v4"              # Name of the Gymnasium environment
  wrapper: None       # Wrapper for the environment (e.g., DreamWrapper or None)
  history: None
  n_future_steps: None        # Number of future steps for the wrapper (if using DreamWrapper)
  n_steps: None             # Number of steps in each environment rollout (for training)
  p_hidden: [256, 128, 64, 32] #384, 17
  d_hidden: [2048, 1024, 512] #17+384,  ?, 384

ppo_hyperparameters:
  policy: 'MlpPolicy'
  batch_size: 256
  n_steps: 512
  device: 'cpu'
  gamma: 0.95
  learning_rate: 3.56987e-05
  ent_coef: 0.00238306
  clip_range: 0.3
  n_epochs: 5
  gae_lambda: 0.9
  max_grad_norm: 2
  vf_coef: 0.431892
  verbose: 1
  policy_kwargs:
    log_std_init: -2
    ortho_init: False
    activation_fn: "ReLU"
    net_arch:
      pi: [256, 256]
      vf: [256, 256]

training:
  seed: None  
  total_timesteps: 1_00_00_000    # Total number of timesteps to train the model
  save_wrapper_state_path: "dreamer_state_dict.pth" # Path to save the dreamer model state