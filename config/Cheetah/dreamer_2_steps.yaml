name: dreamer_2_steps

environment:
  name: "Humanoid-v5"              # Name of the Gymnasium environment
  wrapper: DreamWrapper       # Wrapper for the environment (e.g., DreamWrapper or None)
  n_future_steps: 2         # Number of future steps for the wrapper (if using DreamWrapper)
  n_steps: 1024                # Number of steps in each environment rollout (for training)

ppo_hyperparameters:
  policy: 'MlpPolicy'
  batch_size: 256
  n_steps: 512
  device: 'cpu'
  gamma: 0.95
  learning_rate: 3.56987e-05
  ent_coef: 0.00238306
  clip_range: 0.3
  n_epochs: 5
  gae_lambda: 0.9
  max_grad_norm: 2
  vf_coef: 0.431892
  verbose: 1
  policy_kwargs:
    log_std_init: -2
    ortho_init: False
    activation_fn: "ReLU"
    net_arch:
      pi: [256, 256]
      vf: [256, 256]

training:
  total_timesteps: 1_00_00_000    # Total number of timesteps to train the model
  save_wrapper_state_path: "dreamer_state_dict.pth" # Path to save the dreamer model state
